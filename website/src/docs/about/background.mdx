---
title: Background
---
import CookieBanner from "../../components/cookies"
import OutboundLink from "../../components/outbound-link"
import Collapse from "../../components/collapse"
import License from "../../components/license"

<CookieBanner />

## What are cards?

Cards are short documents that contain information about aspects of machine learning (ML) systems. They generally follow a template divided into several standardised sections and data fields, providing users with boundaries for information input. The concept of a model card to accompany trained ML models was introduced by Google in 2018 with their [Model Cards for Model Reporting](https://arxiv.org/abs/1810.03993) framework. The motivation of their work was to increase transparency around model performance and limitations, and drew from similar documentation initiatives for datasets like [Datasheets for Datasets](https://arxiv.org/abs/1803.09010) and [Data Statements for Natural Language Processing](https://aclanthology.org/Q18-1041/).

These early initiatives inspired other organisations to create their own card-based documentation, widening the landscape of ML reporting artefacts and establishing new methods and framings for information capture. Prominent artefacts developed since 2018 include [Hugging Face’s Model Cards](https://huggingface.co/docs/hub/en/model-cards), [Google’s Data Cards](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533231), [IBM’s AI Factsheets 360](https://aifs360.res.ibm.com/), and [Meta’s System Cards](https://ai.meta.com/research/publications/system-level-transparency-of-machine-learning/). Cards vary in their scope, technicality, intended users and subject matter, but generally focus on one of the three core technological components within the ML lifecycle: data, ML models, and ML systems. A brief history and non-exhaustive list of ML documentation tools has been [shared by Hugging Face researchers](https://huggingface.co/docs/hub/en/model-card-landscape-analysis).

MC+ builds upon the excellent work in this space and is the first initiative that brings technical documentation for datasets, models and systems together into a single package. In doing so, it offers alternatives to existing cards and a wrap-around framing that aims to accommodate other ML documentation initiatives.

## Why did we create MC+?

The MC+ framework was developed by the UK’s Department of Science, Innovation and Technology (DSIT) as a voluntary tool to promote good information governance across AI supply chains. Good information governance—including capturing and sharing dependable information about AI systems—is a vital precursor to other responsible AI initiatives:

1. **Managing risks**. Many risks associated with using AI systems can be traced back to and addressed at the development stage. By breaking down an AI system into its component parts and processes, MC+ could help enable more informed risk assessments, prompt more effective risk treatment, and lead to better outcomes.
2. **Tackling accountability challenges**. With AI systems being used to support decision-making across society, challenging questions about who takes responsibility for the consequences of AI-influenced decisions have emerged. By providing a simple end-to-end documentation framework spanning the AI lifecycle, MC+ could help provide a traceable chain of information ownership, and encourage users to reflect on the responsibilities they have in a broader context of AI delivery.
3. **Providing a foundation for technical assurance**: MC+ encourages developers to record the results of measurements and evaluations performed on datasets, models and systems. Each of these technologies are the subjects—or potential subjects—of standards, regulations and professional assessments that use measurements to define acceptable limits. By providing a focal point for AI evaluation and compliance efforts, MC+ could help industry to engage further with AI standards, regulations and professional assessments.

## How did we create MC+?

The MC+ framework co-opts and extends the principles and characteristics of existing AI cards, and has been further shaped by related DSIT work, external regulatory frameworks and initiatives, and feedback from end-user testing.

Like ML documentation initiatives before, MC+ draws heavily from Google’s initial model and data card work, particularly their organising principles. Our MC+ framework make use of an intuitive ‘block’ layout for displaying data fields – where unit blocks consist of a field name, prompt and space for additional instructions—and aims to observe the [five principles](https://dl.acm.org/doi/fullHtml/10.1145/3531146.3533231#sec-11) that inform the design of the Google Data Card, rewritten in Table 1.

| Five design principles for MC+, adapted from Google's Data Cards (Pushkarna et. al., 2022)|
| --- |
| **P1. Flexible**: describe a wide range of technologies. |
| **P2. Modular**: organise documentation into meaningful sections that are self-contained and well-structured units. |
| **P3. Extensible**: components that can be easily reconfigured or extended systematically for novel technologies, analyses, and platforms. |
| **P4. Accessible**: represent content at multiple granularities so readers can efficiently find and effectively navigate detailed descriptions of the technology. |
| **P5. Content-agnostic**: support diverse media including multiple choice selections, long-form inputs, text, visualisations, images, code blocks, tables, and other interactive elements. |

The data fields of MC+ closely resemble fields in other cards, particularly those from Google and Hugging Face. Some are taken directly from other cards where useful categorisations have been established, and some are variants that have been updated to align with other initiatives or pre-empt development of more advanced digital MC+-related tools.

The MC+ framework has synergies with the UK’s [Algorithmic Transparency Recording Standard](https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub) (ATRS): a communication framework that helps public sector organisations provide clear information about the algorithmic tools they use, and why they’re using them. The three-part structure of technical reporting is replicated in the ATRS framework, such that information provided by suppliers using MC+ cards could be easily ported into an ATRS record by a public body that has procured an AI system. MC+ is also a key part of DSIT’s current AI assurance work. It builds upon the [Introduction to AI Assurance,](https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance) sits alongside the [AI Management Essentials](https://www.gov.uk/government/consultations/ai-management-essentials-tool) tool currently under development following public consultation, and is likely to form part of the recently announced [AI Assurance Platform](https://www.gov.uk/government/publications/assuring-a-responsible-future-for-ai/assuring-a-responsible-future-for-ai#action-1-ai-assurance-platform).

Analysis of the wider regulatory and assurance landscape have also shaped MC+ framework. While MC+ is a voluntary initiative, it aims to prepare businesses to engage with mandatory standards and regulations. As such, we have ensured that successful use of MC+ would equate to meaningful progress towards compliance with the technical documentation requirements of the [EU AI Act](https://artificialintelligenceact.eu/annex/4/) and [ISO 42001](https://www.iso.org/standard/81230.html). Industry initiatives that have influenced MC+ design to date include the TechWorks [Trustable AI Bill Of Materials (TAIBOM)](https://taibom.org/) project that aims to address the challenge of providing formal descriptions of AI systems and dependencies, and various AI governance platforms that demonstrate ways of recording and managing AI information in practice.

Finally, MC+ has undergone one major design iteration following two end-user testing workshops convened by the Ministry of Defence.